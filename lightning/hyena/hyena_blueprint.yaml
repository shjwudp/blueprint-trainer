What It Is:
  Hyena training blueprint

model:
  name: "hyena"
  type: "SimpleLMHeadModel"
  config:
    d_model: 768
    d_inner: 3072
    n_layer: 12
    vocab_size: ${dataset.vocab_size}
    max_position_embeddings: 0
    resid_dropout: 0.0
    embed_dropout: 0.1
    layer_norm_epsilon: 0.00001
    pad_vocab_size_multiple: 1
    layer:
      _name_: "hyena"
      l_max: ${eval:'${dataset.input_seq_len} + 2'}
      order: 2
      filter_order: 128
      num_heads: 1
      inner_factor: 1
      num_blocks: 1
      outer_mixing: False
      drop_rate: 0.15
      filter_dropout: 0.0
      filter_cls: 'hyena-filter'
      post_order_ffn: False
      short_filter_order: 3
      activation_type: "id"
      return_state: False
      filter_args:
          emb_dim: 5 # dim of input to MLP, augments with positional encoding
          w: 10  # frequency of periodic activations (note filter configs say 1, default in object is 10, and final hydra filter indicates 10)
          use_bias: True
          num_inner_mlps: 2
          normalized: False
optimizer: "AdamW"
learning_rate:
  base: 1e-4
  scheduler: "inverse_sqrt"
  num_warmup_steps: 100

dataset:
  type: "icl_synthetics"
  data_dir: "./icl_synthetics"
  num_examples: 4000
  num_test_examples: 500
  vocab_size: 30
  input_seq_len: 16
  copy_method: "assoc_recall"
  seed: 0

batch_size_plan:
- batch_size: 32
  n_step_of_gradient_accumulation: 32
  training_nsteps: -1

logging:
  path: "./hyena_training_log"
  interval_by_step: 1
  interval_by_time: "1h"

checkpoint:
  path: "./hyena_checkpoints"
  interval_by_step: 100
  interval_by_time: "1h"

evaluation:
  interval_by_step: 100
  interval_by_time: "1h"
